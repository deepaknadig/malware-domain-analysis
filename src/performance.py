import re
with open("splitedKeyword.txt") as f:
    content = f.readlines()
#list of delimiters in the data file
delimiters = ['\n', ' ', '/', ',', '.', ':', '!', '$', '?', '%', ';', '@', '-', '_']
words = content
#print(words)
for delimiter in delimiters:
    new_words = []
    for word in words:
        new_words += word.split(delimiter)
    words = new_words
print(words)

#kmeans algorithm
import csv
import re
import pandas as pd
#df = pd.read_csv('splitKeyword.txt')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

#documents = df.values.tolist()
#fo = open("splitedKeyword.txt", "r")
#documents = fo.readlines()
#print(documents)

#documents = ["This little kitty came to play when I was eating at a restaurant.",
 #            "Merley has the best squooshy kitten belly.",
  #          "If you open 100 tab in google you get a smiley face.",
   #          "Best cat photo I've ever taken.",
    #         "Climbing ninja cat.",
     #        "Impressed with google map feedback.",
      #       "Key promoter extension for Google Chrome."]

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(words)
print(X)
X = X.toarray()
#print(X)

from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score
kmeans = KMeans(n_clusters=3, random_state=1).fit(X)
labels = kmeans.labels_
davies_bouldin_score(X, labels)
