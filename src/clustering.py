#splitting the keywords
import re
with open("splitedKeyword.txt") as f:
    content = f.readlines()
#list of delimiters in the data file
delimiters = ['\n', ' ', '/', ',', '.', ':', '!', '$', '?', '%', ';', '@', '-', '_']
words = content
#print(words)
for delimiter in delimiters:
    new_words = []
    for word in words:
        new_words += word.split(delimiter)
    words = new_words
#print(words)

#kmeans algorithm
import csv
import re
import pandas as pd
#df = pd.read_csv('splitKeyword.txt')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

#tokenization
vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(words)
true_k = 6
model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)
model.fit(X)

print("Top terms per cluster:")
order_centroids = model.cluster_centers_.argsort()[:, ::-1]
terms = vectorizer.get_feature_names()
for i in range(true_k):
    print("Cluster %d:" % i),
    for ind in order_centroids[i, :10]:
        print(' %s' % terms[ind]),
    print

print("\n")
print("Prediction")

Y = vectorizer.transform(["chrome browser to open."])
prediction = model.predict(Y)
print(prediction)

Y = vectorizer.transform(["My cat is hungry."])
prediction = model.predict(Y)
print(prediction)
